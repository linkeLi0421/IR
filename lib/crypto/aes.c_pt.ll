; ModuleID = '/llk/IR/lib/crypto/aes.c_pt.bc'
source_filename = "../lib/crypto/aes.c"
target datalayout = "e-m:e-p:32:32-Fi8-i64:64-v128:64:128-a:0:32-n32-S64"
target triple = "armv7-unknown-linux-gnueabi"

module asm ".syntax unified"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_crypto_aes_sbox:\09\09\09\09\09"
module asm "\09.asciz \09\22crypto_aes_sbox\22\09\09\09\09\09"
module asm "__kstrtabns_crypto_aes_sbox:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_crypto_aes_inv_sbox:\09\09\09\09\09"
module asm "\09.asciz \09\22crypto_aes_inv_sbox\22\09\09\09\09\09"
module asm "__kstrtabns_crypto_aes_inv_sbox:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_aes_expandkey:\09\09\09\09\09"
module asm "\09.asciz \09\22aes_expandkey\22\09\09\09\09\09"
module asm "__kstrtabns_aes_expandkey:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_aes_encrypt:\09\09\09\09\09"
module asm "\09.asciz \09\22aes_encrypt\22\09\09\09\09\09"
module asm "__kstrtabns_aes_encrypt:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"
module asm "\09.section \22__ksymtab_strings\22,\22aMS\22,%progbits,1\09"
module asm "__kstrtab_aes_decrypt:\09\09\09\09\09"
module asm "\09.asciz \09\22aes_decrypt\22\09\09\09\09\09"
module asm "__kstrtabns_aes_decrypt:\09\09\09\09\09"
module asm "\09.asciz \09\22\22\09\09\09\09\09"
module asm "\09.previous\09\09\09\09\09\09"

%struct.kernel_symbol = type { i32, ptr, ptr }
%struct.crypto_aes_ctx = type { [60 x i32], [60 x i32], i32 }

@aes_sbox = internal constant [256 x i8] c"c|w{\F2ko\C50\01g+\FE\D7\ABv\CA\82\C9}\FAYG\F0\AD\D4\A2\AF\9C\A4r\C0\B7\FD\93&6?\F7\CC4\A5\E5\F1q\D81\15\04\C7#\C3\18\96\05\9A\07\12\80\E2\EB'\B2u\09\83,\1A\1BnZ\A0R;\D6\B3)\E3/\84S\D1\00\ED \FC\B1[j\CB\BE9JLX\CF\D0\EF\AA\FBCM3\85E\F9\02\7FP<\9F\A8Q\A3@\8F\92\9D8\F5\BC\B6\DA!\10\FF\F3\D2\CD\0C\13\EC_\97D\17\C4\A7~=d]\19s`\81O\DC\22*\90\88F\EE\B8\14\DE^\0B\DB\E02:\0AI\06$\\\C2\D3\ACb\91\95\E4y\E7\C87m\8D\D5N\A9lV\F4\EAez\AE\08\BAx%.\1C\A6\B4\C6\E8\DDt\1FK\BD\8B\8Ap>\B5fH\03\F6\0Ea5W\B9\86\C1\1D\9E\E1\F8\98\11i\D9\8E\94\9B\1E\87\E9\CEU(\DF\8C\A1\89\0D\BF\E6BhA\99-\0F\B0T\BB\16", section ".data..cacheline_aligned", align 64
@aes_inv_sbox = internal constant [256 x i8] c"R\09j\D506\A58\BF@\A3\9E\81\F3\D7\FB|\E39\82\9B/\FF\874\8ECD\C4\DE\E9\CBT{\942\A6\C2#=\EEL\95\0BB\FA\C3N\08.\A1f(\D9$\B2v[\A2Im\8B\D1%r\F8\F6d\86h\98\16\D4\A4\\\CC]e\B6\92lpHP\FD\ED\B9\DA^\15FW\A7\8D\9D\84\90\D8\AB\00\8C\BC\D3\0A\F7\E4X\05\B8\B3E\06\D0,\1E\8F\CA?\0F\02\C1\AF\BD\03\01\13\8Ak:\91\11AOg\DC\EA\97\F2\CF\CE\F0\B4\E6s\96\ACt\22\E7\AD5\85\E2\F97\E8\1Cu\DFnG\F1\1Aq\1D)\C5\89o\B7b\0E\AA\18\BE\1B\FCV>K\C6\D2y \9A\DB\C0\FEx\CDZ\F4\1F\DD\A83\88\07\C71\B1\12\10Y'\80\EC_`Q\7F\A9\19\B5J\0D-\E5z\9F\93\C9\9C\EF\A0\E0;M\AE*\F5\B0\C8\EB\BB<\83S\99a\17+\04~\BAw\D6&\E1i\14cU!\0C}", section ".data..cacheline_aligned", align 64
@__kstrtab_crypto_aes_sbox = external dso_local constant [0 x i8], align 1
@__kstrtabns_crypto_aes_sbox = external dso_local constant [0 x i8], align 1
@__ksymtab_crypto_aes_sbox = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @aes_sbox to i32), ptr @__kstrtab_crypto_aes_sbox, ptr @__kstrtabns_crypto_aes_sbox }, section "___ksymtab+crypto_aes_sbox", align 4
@__kstrtab_crypto_aes_inv_sbox = external dso_local constant [0 x i8], align 1
@__kstrtabns_crypto_aes_inv_sbox = external dso_local constant [0 x i8], align 1
@__ksymtab_crypto_aes_inv_sbox = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @aes_inv_sbox to i32), ptr @__kstrtab_crypto_aes_inv_sbox, ptr @__kstrtabns_crypto_aes_inv_sbox }, section "___ksymtab+crypto_aes_inv_sbox", align 4
@__kstrtab_aes_expandkey = external dso_local constant [0 x i8], align 1
@__kstrtabns_aes_expandkey = external dso_local constant [0 x i8], align 1
@__ksymtab_aes_expandkey = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @aes_expandkey to i32), ptr @__kstrtab_aes_expandkey, ptr @__kstrtabns_aes_expandkey }, section "___ksymtab+aes_expandkey", align 4
@__kstrtab_aes_encrypt = external dso_local constant [0 x i8], align 1
@__kstrtabns_aes_encrypt = external dso_local constant [0 x i8], align 1
@__ksymtab_aes_encrypt = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @aes_encrypt to i32), ptr @__kstrtab_aes_encrypt, ptr @__kstrtabns_aes_encrypt }, section "___ksymtab+aes_encrypt", align 4
@__kstrtab_aes_decrypt = external dso_local constant [0 x i8], align 1
@__kstrtabns_aes_decrypt = external dso_local constant [0 x i8], align 1
@__ksymtab_aes_decrypt = internal constant %struct.kernel_symbol { i32 ptrtoint (ptr @aes_decrypt to i32), ptr @__kstrtab_aes_decrypt, ptr @__kstrtabns_aes_decrypt }, section "___ksymtab+aes_decrypt", align 4
@__UNIQUE_ID_description104 = internal constant [32 x i8] c"description=Generic AES library\00", section ".modinfo", align 1
@__UNIQUE_ID_author105 = internal constant [50 x i8] c"author=Ard Biesheuvel <ard.biesheuvel@linaro.org>\00", section ".modinfo", align 1
@__UNIQUE_ID_license106 = internal constant [15 x i8] c"license=GPL v2\00", section ".modinfo", align 1
@llvm.compiler.used = appending global [8 x ptr] [ptr @__UNIQUE_ID_author105, ptr @__UNIQUE_ID_description104, ptr @__UNIQUE_ID_license106, ptr @__ksymtab_aes_decrypt, ptr @__ksymtab_aes_encrypt, ptr @__ksymtab_aes_expandkey, ptr @__ksymtab_crypto_aes_inv_sbox, ptr @__ksymtab_crypto_aes_sbox], section "llvm.metadata"

@crypto_aes_sbox = dso_local alias [256 x i8], ptr @aes_sbox
@crypto_aes_inv_sbox = dso_local alias [256 x i8], ptr @aes_inv_sbox

; Function Attrs: nofree nounwind null_pointer_is_valid sspstrong uwtable(sync)
define dso_local i32 @aes_expandkey(ptr noundef %0, ptr nocapture noundef readonly %1, i32 noundef %2) #0 {
  %4 = lshr i32 %2, 2
  switch i32 %2, label %281 [
    i32 16, label %5
    i32 24, label %5
    i32 32, label %5
  ]

5:                                                ; preds = %3, %3, %3
  %6 = getelementptr inbounds %struct.crypto_aes_ctx, ptr %0, i32 0, i32 2
  store i32 %2, ptr %6, align 4
  br label %9

7:                                                ; preds = %9
  %8 = add nsw i32 %4, -1
  br label %17

9:                                                ; preds = %9, %5
  %10 = phi i32 [ %15, %9 ], [ 0, %5 ]
  %11 = shl i32 %10, 2
  %12 = getelementptr i8, ptr %1, i32 %11
  %13 = load i32, ptr %12, align 1
  %14 = getelementptr [60 x i32], ptr %0, i32 0, i32 %10
  store i32 %13, ptr %14, align 4
  %15 = add nuw nsw i32 %10, 1
  %16 = icmp eq i32 %15, %4
  br i1 %16, label %7, label %9

17:                                               ; preds = %117, %7
  %18 = phi i32 [ 0, %7 ], [ %118, %117 ]
  %19 = phi i32 [ 1, %7 ], [ %124, %117 ]
  %20 = mul i32 %18, %4
  %21 = getelementptr i32, ptr %0, i32 %20
  %22 = getelementptr i32, ptr %21, i32 %4
  %23 = getelementptr i32, ptr %21, i32 %8
  %24 = load i32, ptr %23, align 4
  %25 = and i32 %24, 255
  %26 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %25
  %27 = load volatile i8, ptr %26, align 1
  %28 = zext i8 %27 to i32
  %29 = lshr i32 %24, 8
  %30 = and i32 %29, 255
  %31 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %30
  %32 = load volatile i8, ptr %31, align 1
  %33 = zext i8 %32 to i32
  %34 = shl nuw nsw i32 %33, 8
  %35 = lshr i32 %24, 16
  %36 = and i32 %35, 255
  %37 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %36
  %38 = load volatile i8, ptr %37, align 1
  %39 = zext i8 %38 to i32
  %40 = shl nuw nsw i32 %39, 16
  %41 = or i32 %40, %34
  %42 = lshr i32 %24, 24
  %43 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %42
  %44 = load volatile i8, ptr %43, align 1
  %45 = zext i8 %44 to i32
  %46 = shl nuw i32 %45, 24
  %47 = or i32 %41, %46
  %48 = tail call i32 @llvm.fshl.i32(i32 %28, i32 %47, i32 24)
  %49 = load i32, ptr %21, align 4
  %50 = xor i32 %49, %19
  %51 = xor i32 %50, %48
  store i32 %51, ptr %22, align 4
  %52 = getelementptr i32, ptr %21, i32 1
  %53 = load i32, ptr %52, align 4
  %54 = xor i32 %51, %53
  %55 = getelementptr i32, ptr %22, i32 1
  store i32 %54, ptr %55, align 4
  %56 = getelementptr i32, ptr %21, i32 2
  %57 = load i32, ptr %56, align 4
  %58 = xor i32 %54, %57
  %59 = getelementptr i32, ptr %22, i32 2
  store i32 %58, ptr %59, align 4
  %60 = getelementptr i32, ptr %21, i32 3
  %61 = load i32, ptr %60, align 4
  %62 = xor i32 %58, %61
  %63 = getelementptr i32, ptr %22, i32 3
  store i32 %62, ptr %63, align 4
  switch i32 %2, label %117 [
    i32 24, label %64
    i32 32, label %69
  ]

64:                                               ; preds = %17
  %65 = icmp ugt i32 %18, 6
  br i1 %65, label %126, label %66

66:                                               ; preds = %64
  %67 = getelementptr i32, ptr %21, i32 4
  %68 = load i32, ptr %67, align 4
  br label %106

69:                                               ; preds = %17
  %70 = icmp ugt i32 %18, 5
  br i1 %70, label %126, label %71

71:                                               ; preds = %69
  %72 = and i32 %62, 255
  %73 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %72
  %74 = load volatile i8, ptr %73, align 1
  %75 = zext i8 %74 to i32
  %76 = lshr i32 %62, 8
  %77 = and i32 %76, 255
  %78 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %77
  %79 = load volatile i8, ptr %78, align 1
  %80 = zext i8 %79 to i32
  %81 = shl nuw nsw i32 %80, 8
  %82 = or i32 %81, %75
  %83 = lshr i32 %62, 16
  %84 = and i32 %83, 255
  %85 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %84
  %86 = load volatile i8, ptr %85, align 1
  %87 = zext i8 %86 to i32
  %88 = shl nuw nsw i32 %87, 16
  %89 = or i32 %82, %88
  %90 = lshr i32 %62, 24
  %91 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %90
  %92 = load volatile i8, ptr %91, align 1
  %93 = zext i8 %92 to i32
  %94 = shl nuw i32 %93, 24
  %95 = or i32 %89, %94
  %96 = getelementptr i32, ptr %21, i32 4
  %97 = load i32, ptr %96, align 4
  %98 = xor i32 %95, %97
  %99 = getelementptr i32, ptr %22, i32 4
  store i32 %98, ptr %99, align 4
  %100 = getelementptr i32, ptr %21, i32 5
  %101 = load i32, ptr %100, align 4
  %102 = xor i32 %98, %101
  %103 = getelementptr i32, ptr %22, i32 5
  store i32 %102, ptr %103, align 4
  %104 = getelementptr i32, ptr %21, i32 6
  %105 = load i32, ptr %104, align 4
  br label %106

106:                                              ; preds = %71, %66
  %107 = phi i32 [ %62, %66 ], [ %105, %71 ]
  %108 = phi i32 [ %68, %66 ], [ %102, %71 ]
  %109 = phi i32 [ 4, %66 ], [ 6, %71 ]
  %110 = phi i32 [ 5, %66 ], [ 7, %71 ]
  %111 = xor i32 %108, %107
  %112 = getelementptr i32, ptr %22, i32 %109
  store i32 %111, ptr %112, align 4
  %113 = getelementptr i32, ptr %21, i32 %110
  %114 = load i32, ptr %113, align 4
  %115 = xor i32 %114, %111
  %116 = getelementptr i32, ptr %22, i32 %110
  store i32 %115, ptr %116, align 4
  br label %117

117:                                              ; preds = %106, %17
  %118 = add nuw nsw i32 %18, 1
  %119 = shl i32 %19, 1
  %120 = and i32 %119, -16843010
  %121 = lshr i32 %19, 7
  %122 = and i32 %121, 16843009
  %123 = mul nuw nsw i32 %122, 27
  %124 = xor i32 %123, %120
  %125 = icmp eq i32 %118, 10
  br i1 %125, label %126, label %17

126:                                              ; preds = %117, %69, %64
  %127 = add i32 %2, 24
  %128 = getelementptr [60 x i32], ptr %0, i32 0, i32 %127
  %129 = load i32, ptr %128, align 4
  %130 = getelementptr inbounds %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1
  store i32 %129, ptr %130, align 4
  %131 = add i32 %2, 25
  %132 = getelementptr [60 x i32], ptr %0, i32 0, i32 %131
  %133 = load i32, ptr %132, align 4
  %134 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 1
  store i32 %133, ptr %134, align 4
  %135 = add i32 %2, 26
  %136 = getelementptr [60 x i32], ptr %0, i32 0, i32 %135
  %137 = load i32, ptr %136, align 4
  %138 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 2
  store i32 %137, ptr %138, align 4
  %139 = add i32 %2, 27
  %140 = getelementptr [60 x i32], ptr %0, i32 0, i32 %139
  %141 = load i32, ptr %140, align 4
  %142 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 3
  store i32 %141, ptr %142, align 4
  %143 = add i32 %2, 20
  %144 = icmp eq i32 %143, 0
  br i1 %144, label %265, label %145

145:                                              ; preds = %145, %126
  %146 = phi i32 [ %263, %145 ], [ %143, %126 ]
  %147 = phi i32 [ %262, %145 ], [ 4, %126 ]
  %148 = getelementptr [60 x i32], ptr %0, i32 0, i32 %146
  %149 = load i32, ptr %148, align 4
  %150 = shl i32 %149, 2
  %151 = and i32 %150, -50529028
  %152 = lshr i32 %149, 7
  %153 = and i32 %152, 16843009
  %154 = mul nuw nsw i32 %153, 54
  %155 = xor i32 %154, %151
  %156 = lshr i32 %149, 6
  %157 = and i32 %156, 16843009
  %158 = mul nuw nsw i32 %157, 27
  %159 = xor i32 %155, %158
  %160 = xor i32 %159, %149
  %161 = tail call i32 @llvm.fshl.i32(i32 %159, i32 %159, i32 16) #2
  %162 = xor i32 %160, %161
  %163 = shl i32 %162, 1
  %164 = and i32 %163, -16843010
  %165 = lshr i32 %162, 7
  %166 = and i32 %165, 16843009
  %167 = mul nuw nsw i32 %166, 27
  %168 = tail call i32 @llvm.fshl.i32(i32 %162, i32 %162, i32 16) #2
  %169 = xor i32 %164, %168
  %170 = xor i32 %169, %167
  %171 = xor i32 %170, %162
  %172 = tail call i32 @llvm.fshl.i32(i32 %171, i32 %171, i32 24) #2
  %173 = xor i32 %172, %170
  %174 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %147
  store i32 %173, ptr %174, align 4
  %175 = add i32 %146, 1
  %176 = getelementptr [60 x i32], ptr %0, i32 0, i32 %175
  %177 = load i32, ptr %176, align 4
  %178 = shl i32 %177, 2
  %179 = and i32 %178, -50529028
  %180 = lshr i32 %177, 7
  %181 = and i32 %180, 16843009
  %182 = mul nuw nsw i32 %181, 54
  %183 = xor i32 %182, %179
  %184 = lshr i32 %177, 6
  %185 = and i32 %184, 16843009
  %186 = mul nuw nsw i32 %185, 27
  %187 = xor i32 %183, %186
  %188 = xor i32 %187, %177
  %189 = tail call i32 @llvm.fshl.i32(i32 %187, i32 %187, i32 16) #2
  %190 = xor i32 %188, %189
  %191 = shl i32 %190, 1
  %192 = and i32 %191, -16843010
  %193 = lshr i32 %190, 7
  %194 = and i32 %193, 16843009
  %195 = mul nuw nsw i32 %194, 27
  %196 = tail call i32 @llvm.fshl.i32(i32 %190, i32 %190, i32 16) #2
  %197 = xor i32 %192, %196
  %198 = xor i32 %197, %195
  %199 = xor i32 %198, %190
  %200 = tail call i32 @llvm.fshl.i32(i32 %199, i32 %199, i32 24) #2
  %201 = xor i32 %200, %198
  %202 = or i32 %147, 1
  %203 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %202
  store i32 %201, ptr %203, align 4
  %204 = add i32 %146, 2
  %205 = getelementptr [60 x i32], ptr %0, i32 0, i32 %204
  %206 = load i32, ptr %205, align 4
  %207 = shl i32 %206, 2
  %208 = and i32 %207, -50529028
  %209 = lshr i32 %206, 7
  %210 = and i32 %209, 16843009
  %211 = mul nuw nsw i32 %210, 54
  %212 = xor i32 %211, %208
  %213 = lshr i32 %206, 6
  %214 = and i32 %213, 16843009
  %215 = mul nuw nsw i32 %214, 27
  %216 = xor i32 %212, %215
  %217 = xor i32 %216, %206
  %218 = tail call i32 @llvm.fshl.i32(i32 %216, i32 %216, i32 16) #2
  %219 = xor i32 %217, %218
  %220 = shl i32 %219, 1
  %221 = and i32 %220, -16843010
  %222 = lshr i32 %219, 7
  %223 = and i32 %222, 16843009
  %224 = mul nuw nsw i32 %223, 27
  %225 = tail call i32 @llvm.fshl.i32(i32 %219, i32 %219, i32 16) #2
  %226 = xor i32 %221, %225
  %227 = xor i32 %226, %224
  %228 = xor i32 %227, %219
  %229 = tail call i32 @llvm.fshl.i32(i32 %228, i32 %228, i32 24) #2
  %230 = xor i32 %229, %227
  %231 = or i32 %147, 2
  %232 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %231
  store i32 %230, ptr %232, align 4
  %233 = add i32 %146, 3
  %234 = getelementptr [60 x i32], ptr %0, i32 0, i32 %233
  %235 = load i32, ptr %234, align 4
  %236 = shl i32 %235, 2
  %237 = and i32 %236, -50529028
  %238 = lshr i32 %235, 7
  %239 = and i32 %238, 16843009
  %240 = mul nuw nsw i32 %239, 54
  %241 = xor i32 %240, %237
  %242 = lshr i32 %235, 6
  %243 = and i32 %242, 16843009
  %244 = mul nuw nsw i32 %243, 27
  %245 = xor i32 %241, %244
  %246 = xor i32 %245, %235
  %247 = tail call i32 @llvm.fshl.i32(i32 %245, i32 %245, i32 16) #2
  %248 = xor i32 %246, %247
  %249 = shl i32 %248, 1
  %250 = and i32 %249, -16843010
  %251 = lshr i32 %248, 7
  %252 = and i32 %251, 16843009
  %253 = mul nuw nsw i32 %252, 27
  %254 = tail call i32 @llvm.fshl.i32(i32 %248, i32 %248, i32 16) #2
  %255 = xor i32 %250, %254
  %256 = xor i32 %255, %253
  %257 = xor i32 %256, %248
  %258 = tail call i32 @llvm.fshl.i32(i32 %257, i32 %257, i32 24) #2
  %259 = xor i32 %258, %256
  %260 = or i32 %147, 3
  %261 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %260
  store i32 %259, ptr %261, align 4
  %262 = add i32 %147, 4
  %263 = add i32 %146, -4
  %264 = icmp eq i32 %263, 0
  br i1 %264, label %265, label %145

265:                                              ; preds = %145, %126
  %266 = phi i32 [ 4, %126 ], [ %262, %145 ]
  %267 = load i32, ptr %0, align 4
  %268 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %266
  store i32 %267, ptr %268, align 4
  %269 = getelementptr [60 x i32], ptr %0, i32 0, i32 1
  %270 = load i32, ptr %269, align 4
  %271 = or i32 %266, 1
  %272 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %271
  store i32 %270, ptr %272, align 4
  %273 = getelementptr [60 x i32], ptr %0, i32 0, i32 2
  %274 = load i32, ptr %273, align 4
  %275 = or i32 %266, 2
  %276 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %275
  store i32 %274, ptr %276, align 4
  %277 = getelementptr [60 x i32], ptr %0, i32 0, i32 3
  %278 = load i32, ptr %277, align 4
  %279 = or i32 %266, 3
  %280 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 %279
  store i32 %278, ptr %280, align 4
  br label %281

281:                                              ; preds = %265, %3
  %282 = phi i32 [ 0, %265 ], [ -22, %3 ]
  ret i32 %282
}

; Function Attrs: nofree nounwind null_pointer_is_valid sspstrong uwtable(sync)
define dso_local void @aes_encrypt(ptr nocapture noundef readonly %0, ptr nocapture noundef writeonly %1, ptr nocapture noundef readonly %2) #0 {
  %4 = getelementptr i32, ptr %0, i32 4
  %5 = getelementptr inbounds %struct.crypto_aes_ctx, ptr %0, i32 0, i32 2
  %6 = load i32, ptr %5, align 4
  %7 = lshr i32 %6, 2
  %8 = load i32, ptr %0, align 4
  %9 = load i32, ptr %2, align 1
  %10 = xor i32 %9, %8
  %11 = getelementptr [60 x i32], ptr %0, i32 0, i32 1
  %12 = load i32, ptr %11, align 4
  %13 = getelementptr i8, ptr %2, i32 4
  %14 = load i32, ptr %13, align 1
  %15 = xor i32 %14, %12
  %16 = getelementptr [60 x i32], ptr %0, i32 0, i32 2
  %17 = load i32, ptr %16, align 4
  %18 = getelementptr i8, ptr %2, i32 8
  %19 = load i32, ptr %18, align 1
  %20 = xor i32 %19, %17
  %21 = getelementptr [60 x i32], ptr %0, i32 0, i32 3
  %22 = load i32, ptr %21, align 4
  %23 = getelementptr i8, ptr %2, i32 12
  %24 = load i32, ptr %23, align 1
  %25 = xor i32 %24, %22
  %26 = load volatile i8, ptr @aes_sbox, align 64
  %27 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 64), align 64
  %28 = xor i8 %27, %26
  %29 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 134), align 2
  %30 = xor i8 %28, %29
  %31 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 195), align 1
  %32 = xor i8 %30, %31
  %33 = zext i8 %32 to i32
  %34 = xor i32 %10, %33
  %35 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 16), align 16
  %36 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 82), align 2
  %37 = xor i8 %36, %35
  %38 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 158), align 2
  %39 = xor i8 %37, %38
  %40 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 221), align 1
  %41 = xor i8 %39, %40
  %42 = zext i8 %41 to i32
  %43 = xor i32 %15, %42
  %44 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 32), align 32
  %45 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 96), align 32
  %46 = xor i8 %45, %44
  %47 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 160), align 32
  %48 = xor i8 %46, %47
  %49 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 234), align 2
  %50 = xor i8 %48, %49
  %51 = zext i8 %50 to i32
  %52 = xor i32 %20, %51
  %53 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 48), align 16
  %54 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 112), align 16
  %55 = xor i8 %54, %53
  %56 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 186), align 2
  %57 = xor i8 %55, %56
  %58 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_sbox, i32 0, i32 241), align 1
  %59 = xor i8 %57, %58
  %60 = zext i8 %59 to i32
  %61 = xor i32 %25, %60
  %62 = add nuw nsw i32 %7, 4
  br label %63

63:                                               ; preds = %246, %3
  %64 = phi i32 [ %34, %3 ], [ %260, %246 ]
  %65 = phi i32 [ %43, %3 ], [ %298, %246 ]
  %66 = phi i32 [ %52, %3 ], [ %336, %246 ]
  %67 = phi i32 [ %61, %3 ], [ %374, %246 ]
  %68 = phi ptr [ %4, %3 ], [ %376, %246 ]
  %69 = phi i32 [ 0, %3 ], [ %375, %246 ]
  %70 = and i32 %64, 255
  %71 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %70
  %72 = load volatile i8, ptr %71, align 1
  %73 = zext i8 %72 to i32
  %74 = lshr i32 %65, 8
  %75 = and i32 %74, 255
  %76 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %75
  %77 = load volatile i8, ptr %76, align 1
  %78 = zext i8 %77 to i32
  %79 = shl nuw nsw i32 %78, 8
  %80 = or i32 %79, %73
  %81 = lshr i32 %66, 16
  %82 = and i32 %81, 255
  %83 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %82
  %84 = load volatile i8, ptr %83, align 1
  %85 = zext i8 %84 to i32
  %86 = shl nuw nsw i32 %85, 16
  %87 = or i32 %80, %86
  %88 = lshr i32 %67, 24
  %89 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %88
  %90 = load volatile i8, ptr %89, align 1
  %91 = zext i8 %90 to i32
  %92 = shl nuw i32 %91, 24
  %93 = or i32 %87, %92
  %94 = shl i32 %93, 1
  %95 = and i32 %94, -16843010
  %96 = lshr i32 %93, 7
  %97 = and i32 %96, 16843009
  %98 = mul nuw nsw i32 %97, 27
  %99 = tail call i32 @llvm.fshl.i32(i32 %80, i32 %93, i32 16) #2
  %100 = xor i32 %95, %99
  %101 = xor i32 %100, %98
  %102 = xor i32 %101, %93
  %103 = tail call i32 @llvm.fshl.i32(i32 %102, i32 %102, i32 24) #2
  %104 = load i32, ptr %68, align 4
  %105 = xor i32 %101, %104
  %106 = xor i32 %105, %103
  %107 = and i32 %65, 255
  %108 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %107
  %109 = load volatile i8, ptr %108, align 1
  %110 = zext i8 %109 to i32
  %111 = lshr i32 %66, 8
  %112 = and i32 %111, 255
  %113 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %112
  %114 = load volatile i8, ptr %113, align 1
  %115 = zext i8 %114 to i32
  %116 = shl nuw nsw i32 %115, 8
  %117 = or i32 %116, %110
  %118 = lshr i32 %67, 16
  %119 = and i32 %118, 255
  %120 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %119
  %121 = load volatile i8, ptr %120, align 1
  %122 = zext i8 %121 to i32
  %123 = shl nuw nsw i32 %122, 16
  %124 = or i32 %117, %123
  %125 = lshr i32 %64, 24
  %126 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %125
  %127 = load volatile i8, ptr %126, align 1
  %128 = zext i8 %127 to i32
  %129 = shl nuw i32 %128, 24
  %130 = or i32 %124, %129
  %131 = shl i32 %130, 1
  %132 = and i32 %131, -16843010
  %133 = lshr i32 %130, 7
  %134 = and i32 %133, 16843009
  %135 = mul nuw nsw i32 %134, 27
  %136 = tail call i32 @llvm.fshl.i32(i32 %117, i32 %130, i32 16) #2
  %137 = xor i32 %132, %136
  %138 = xor i32 %137, %135
  %139 = xor i32 %138, %130
  %140 = tail call i32 @llvm.fshl.i32(i32 %139, i32 %139, i32 24) #2
  %141 = getelementptr i32, ptr %68, i32 1
  %142 = load i32, ptr %141, align 4
  %143 = xor i32 %138, %142
  %144 = xor i32 %143, %140
  %145 = and i32 %66, 255
  %146 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %145
  %147 = load volatile i8, ptr %146, align 1
  %148 = zext i8 %147 to i32
  %149 = lshr i32 %67, 8
  %150 = and i32 %149, 255
  %151 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %150
  %152 = load volatile i8, ptr %151, align 1
  %153 = zext i8 %152 to i32
  %154 = shl nuw nsw i32 %153, 8
  %155 = or i32 %154, %148
  %156 = lshr i32 %64, 16
  %157 = and i32 %156, 255
  %158 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %157
  %159 = load volatile i8, ptr %158, align 1
  %160 = zext i8 %159 to i32
  %161 = shl nuw nsw i32 %160, 16
  %162 = or i32 %155, %161
  %163 = lshr i32 %65, 24
  %164 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %163
  %165 = load volatile i8, ptr %164, align 1
  %166 = zext i8 %165 to i32
  %167 = shl nuw i32 %166, 24
  %168 = or i32 %162, %167
  %169 = shl i32 %168, 1
  %170 = and i32 %169, -16843010
  %171 = lshr i32 %168, 7
  %172 = and i32 %171, 16843009
  %173 = mul nuw nsw i32 %172, 27
  %174 = tail call i32 @llvm.fshl.i32(i32 %155, i32 %168, i32 16) #2
  %175 = xor i32 %170, %174
  %176 = xor i32 %175, %173
  %177 = xor i32 %176, %168
  %178 = tail call i32 @llvm.fshl.i32(i32 %177, i32 %177, i32 24) #2
  %179 = getelementptr i32, ptr %68, i32 2
  %180 = load i32, ptr %179, align 4
  %181 = xor i32 %176, %180
  %182 = xor i32 %181, %178
  %183 = and i32 %67, 255
  %184 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %183
  %185 = load volatile i8, ptr %184, align 1
  %186 = zext i8 %185 to i32
  %187 = lshr i32 %64, 8
  %188 = and i32 %187, 255
  %189 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %188
  %190 = load volatile i8, ptr %189, align 1
  %191 = zext i8 %190 to i32
  %192 = shl nuw nsw i32 %191, 8
  %193 = or i32 %192, %186
  %194 = lshr i32 %65, 16
  %195 = and i32 %194, 255
  %196 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %195
  %197 = load volatile i8, ptr %196, align 1
  %198 = zext i8 %197 to i32
  %199 = shl nuw nsw i32 %198, 16
  %200 = or i32 %193, %199
  %201 = lshr i32 %66, 24
  %202 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %201
  %203 = load volatile i8, ptr %202, align 1
  %204 = zext i8 %203 to i32
  %205 = shl nuw i32 %204, 24
  %206 = or i32 %200, %205
  %207 = shl i32 %206, 1
  %208 = and i32 %207, -16843010
  %209 = lshr i32 %206, 7
  %210 = and i32 %209, 16843009
  %211 = mul nuw nsw i32 %210, 27
  %212 = tail call i32 @llvm.fshl.i32(i32 %193, i32 %206, i32 16) #2
  %213 = xor i32 %208, %212
  %214 = xor i32 %213, %211
  %215 = xor i32 %214, %206
  %216 = tail call i32 @llvm.fshl.i32(i32 %215, i32 %215, i32 24) #2
  %217 = getelementptr i32, ptr %68, i32 3
  %218 = load i32, ptr %217, align 4
  %219 = xor i32 %214, %218
  %220 = xor i32 %219, %216
  %221 = icmp eq i32 %69, %62
  %222 = and i32 %106, 255
  %223 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %222
  %224 = load volatile i8, ptr %223, align 1
  %225 = zext i8 %224 to i32
  %226 = lshr i32 %144, 8
  %227 = and i32 %226, 255
  %228 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %227
  %229 = load volatile i8, ptr %228, align 1
  %230 = zext i8 %229 to i32
  %231 = shl nuw nsw i32 %230, 8
  %232 = or i32 %231, %225
  %233 = lshr i32 %182, 16
  %234 = and i32 %233, 255
  %235 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %234
  %236 = load volatile i8, ptr %235, align 1
  %237 = zext i8 %236 to i32
  %238 = shl nuw nsw i32 %237, 16
  %239 = or i32 %232, %238
  %240 = lshr i32 %220, 24
  %241 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %240
  %242 = load volatile i8, ptr %241, align 1
  %243 = zext i8 %242 to i32
  %244 = shl nuw i32 %243, 24
  %245 = or i32 %239, %244
  br i1 %221, label %377, label %246

246:                                              ; preds = %63
  %247 = shl i32 %245, 1
  %248 = and i32 %247, -16843010
  %249 = lshr i32 %245, 7
  %250 = and i32 %249, 16843009
  %251 = mul nuw nsw i32 %250, 27
  %252 = tail call i32 @llvm.fshl.i32(i32 %232, i32 %245, i32 16) #2
  %253 = xor i32 %248, %252
  %254 = xor i32 %253, %251
  %255 = xor i32 %254, %245
  %256 = tail call i32 @llvm.fshl.i32(i32 %255, i32 %255, i32 24) #2
  %257 = getelementptr i32, ptr %68, i32 4
  %258 = load i32, ptr %257, align 4
  %259 = xor i32 %254, %258
  %260 = xor i32 %259, %256
  %261 = and i32 %144, 255
  %262 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %261
  %263 = load volatile i8, ptr %262, align 1
  %264 = zext i8 %263 to i32
  %265 = lshr i32 %182, 8
  %266 = and i32 %265, 255
  %267 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %266
  %268 = load volatile i8, ptr %267, align 1
  %269 = zext i8 %268 to i32
  %270 = shl nuw nsw i32 %269, 8
  %271 = or i32 %270, %264
  %272 = lshr i32 %220, 16
  %273 = and i32 %272, 255
  %274 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %273
  %275 = load volatile i8, ptr %274, align 1
  %276 = zext i8 %275 to i32
  %277 = shl nuw nsw i32 %276, 16
  %278 = or i32 %271, %277
  %279 = lshr i32 %106, 24
  %280 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %279
  %281 = load volatile i8, ptr %280, align 1
  %282 = zext i8 %281 to i32
  %283 = shl nuw i32 %282, 24
  %284 = or i32 %278, %283
  %285 = shl i32 %284, 1
  %286 = and i32 %285, -16843010
  %287 = lshr i32 %284, 7
  %288 = and i32 %287, 16843009
  %289 = mul nuw nsw i32 %288, 27
  %290 = tail call i32 @llvm.fshl.i32(i32 %271, i32 %284, i32 16) #2
  %291 = xor i32 %286, %290
  %292 = xor i32 %291, %289
  %293 = xor i32 %292, %284
  %294 = tail call i32 @llvm.fshl.i32(i32 %293, i32 %293, i32 24) #2
  %295 = getelementptr i32, ptr %68, i32 5
  %296 = load i32, ptr %295, align 4
  %297 = xor i32 %292, %296
  %298 = xor i32 %297, %294
  %299 = and i32 %182, 255
  %300 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %299
  %301 = load volatile i8, ptr %300, align 1
  %302 = zext i8 %301 to i32
  %303 = lshr i32 %220, 8
  %304 = and i32 %303, 255
  %305 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %304
  %306 = load volatile i8, ptr %305, align 1
  %307 = zext i8 %306 to i32
  %308 = shl nuw nsw i32 %307, 8
  %309 = or i32 %308, %302
  %310 = lshr i32 %106, 16
  %311 = and i32 %310, 255
  %312 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %311
  %313 = load volatile i8, ptr %312, align 1
  %314 = zext i8 %313 to i32
  %315 = shl nuw nsw i32 %314, 16
  %316 = or i32 %309, %315
  %317 = lshr i32 %144, 24
  %318 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %317
  %319 = load volatile i8, ptr %318, align 1
  %320 = zext i8 %319 to i32
  %321 = shl nuw i32 %320, 24
  %322 = or i32 %316, %321
  %323 = shl i32 %322, 1
  %324 = and i32 %323, -16843010
  %325 = lshr i32 %322, 7
  %326 = and i32 %325, 16843009
  %327 = mul nuw nsw i32 %326, 27
  %328 = tail call i32 @llvm.fshl.i32(i32 %309, i32 %322, i32 16) #2
  %329 = xor i32 %324, %328
  %330 = xor i32 %329, %327
  %331 = xor i32 %330, %322
  %332 = tail call i32 @llvm.fshl.i32(i32 %331, i32 %331, i32 24) #2
  %333 = getelementptr i32, ptr %68, i32 6
  %334 = load i32, ptr %333, align 4
  %335 = xor i32 %330, %334
  %336 = xor i32 %335, %332
  %337 = and i32 %220, 255
  %338 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %337
  %339 = load volatile i8, ptr %338, align 1
  %340 = zext i8 %339 to i32
  %341 = lshr i32 %106, 8
  %342 = and i32 %341, 255
  %343 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %342
  %344 = load volatile i8, ptr %343, align 1
  %345 = zext i8 %344 to i32
  %346 = shl nuw nsw i32 %345, 8
  %347 = or i32 %346, %340
  %348 = lshr i32 %144, 16
  %349 = and i32 %348, 255
  %350 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %349
  %351 = load volatile i8, ptr %350, align 1
  %352 = zext i8 %351 to i32
  %353 = shl nuw nsw i32 %352, 16
  %354 = or i32 %347, %353
  %355 = lshr i32 %182, 24
  %356 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %355
  %357 = load volatile i8, ptr %356, align 1
  %358 = zext i8 %357 to i32
  %359 = shl nuw i32 %358, 24
  %360 = or i32 %354, %359
  %361 = shl i32 %360, 1
  %362 = and i32 %361, -16843010
  %363 = lshr i32 %360, 7
  %364 = and i32 %363, 16843009
  %365 = mul nuw nsw i32 %364, 27
  %366 = tail call i32 @llvm.fshl.i32(i32 %347, i32 %360, i32 16) #2
  %367 = xor i32 %362, %366
  %368 = xor i32 %367, %365
  %369 = xor i32 %368, %360
  %370 = tail call i32 @llvm.fshl.i32(i32 %369, i32 %369, i32 24) #2
  %371 = getelementptr i32, ptr %68, i32 7
  %372 = load i32, ptr %371, align 4
  %373 = xor i32 %368, %372
  %374 = xor i32 %373, %370
  %375 = add i32 %69, 2
  %376 = getelementptr i32, ptr %68, i32 8
  br label %63

377:                                              ; preds = %63
  %378 = getelementptr i32, ptr %68, i32 4
  %379 = load i32, ptr %378, align 4
  %380 = xor i32 %245, %379
  store i32 %380, ptr %1, align 1
  %381 = and i32 %144, 255
  %382 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %381
  %383 = load volatile i8, ptr %382, align 1
  %384 = zext i8 %383 to i32
  %385 = lshr i32 %182, 8
  %386 = and i32 %385, 255
  %387 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %386
  %388 = load volatile i8, ptr %387, align 1
  %389 = zext i8 %388 to i32
  %390 = shl nuw nsw i32 %389, 8
  %391 = or i32 %390, %384
  %392 = lshr i32 %220, 16
  %393 = and i32 %392, 255
  %394 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %393
  %395 = load volatile i8, ptr %394, align 1
  %396 = zext i8 %395 to i32
  %397 = shl nuw nsw i32 %396, 16
  %398 = or i32 %391, %397
  %399 = lshr i32 %106, 24
  %400 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %399
  %401 = load volatile i8, ptr %400, align 1
  %402 = zext i8 %401 to i32
  %403 = shl nuw i32 %402, 24
  %404 = or i32 %398, %403
  %405 = getelementptr i32, ptr %68, i32 5
  %406 = load i32, ptr %405, align 4
  %407 = xor i32 %404, %406
  %408 = getelementptr i8, ptr %1, i32 4
  store i32 %407, ptr %408, align 1
  %409 = and i32 %182, 255
  %410 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %409
  %411 = load volatile i8, ptr %410, align 1
  %412 = zext i8 %411 to i32
  %413 = lshr i32 %220, 8
  %414 = and i32 %413, 255
  %415 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %414
  %416 = load volatile i8, ptr %415, align 1
  %417 = zext i8 %416 to i32
  %418 = shl nuw nsw i32 %417, 8
  %419 = or i32 %418, %412
  %420 = lshr i32 %106, 16
  %421 = and i32 %420, 255
  %422 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %421
  %423 = load volatile i8, ptr %422, align 1
  %424 = zext i8 %423 to i32
  %425 = shl nuw nsw i32 %424, 16
  %426 = or i32 %419, %425
  %427 = lshr i32 %144, 24
  %428 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %427
  %429 = load volatile i8, ptr %428, align 1
  %430 = zext i8 %429 to i32
  %431 = shl nuw i32 %430, 24
  %432 = or i32 %426, %431
  %433 = getelementptr i32, ptr %68, i32 6
  %434 = load i32, ptr %433, align 4
  %435 = xor i32 %432, %434
  %436 = getelementptr i8, ptr %1, i32 8
  store i32 %435, ptr %436, align 1
  %437 = and i32 %220, 255
  %438 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %437
  %439 = load volatile i8, ptr %438, align 1
  %440 = zext i8 %439 to i32
  %441 = lshr i32 %106, 8
  %442 = and i32 %441, 255
  %443 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %442
  %444 = load volatile i8, ptr %443, align 1
  %445 = zext i8 %444 to i32
  %446 = shl nuw nsw i32 %445, 8
  %447 = or i32 %446, %440
  %448 = lshr i32 %144, 16
  %449 = and i32 %448, 255
  %450 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %449
  %451 = load volatile i8, ptr %450, align 1
  %452 = zext i8 %451 to i32
  %453 = shl nuw nsw i32 %452, 16
  %454 = or i32 %447, %453
  %455 = lshr i32 %182, 24
  %456 = getelementptr [256 x i8], ptr @aes_sbox, i32 0, i32 %455
  %457 = load volatile i8, ptr %456, align 1
  %458 = zext i8 %457 to i32
  %459 = shl nuw i32 %458, 24
  %460 = or i32 %454, %459
  %461 = getelementptr i32, ptr %68, i32 7
  %462 = load i32, ptr %461, align 4
  %463 = xor i32 %460, %462
  %464 = getelementptr i8, ptr %1, i32 12
  store i32 %463, ptr %464, align 1
  ret void
}

; Function Attrs: nofree nounwind null_pointer_is_valid sspstrong uwtable(sync)
define dso_local void @aes_decrypt(ptr nocapture noundef readonly %0, ptr nocapture noundef writeonly %1, ptr nocapture noundef readonly %2) #0 {
  %4 = getelementptr inbounds %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1
  %5 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 4
  %6 = getelementptr inbounds %struct.crypto_aes_ctx, ptr %0, i32 0, i32 2
  %7 = load i32, ptr %6, align 4
  %8 = lshr i32 %7, 2
  %9 = load i32, ptr %4, align 4
  %10 = load i32, ptr %2, align 1
  %11 = xor i32 %10, %9
  %12 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 1
  %13 = load i32, ptr %12, align 4
  %14 = getelementptr i8, ptr %2, i32 4
  %15 = load i32, ptr %14, align 1
  %16 = xor i32 %15, %13
  %17 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 2
  %18 = load i32, ptr %17, align 4
  %19 = getelementptr i8, ptr %2, i32 8
  %20 = load i32, ptr %19, align 1
  %21 = xor i32 %20, %18
  %22 = getelementptr %struct.crypto_aes_ctx, ptr %0, i32 0, i32 1, i32 3
  %23 = load i32, ptr %22, align 4
  %24 = getelementptr i8, ptr %2, i32 12
  %25 = load i32, ptr %24, align 1
  %26 = xor i32 %25, %23
  %27 = load volatile i8, ptr @aes_inv_sbox, align 64
  %28 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 64), align 64
  %29 = xor i8 %28, %27
  %30 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 129), align 1
  %31 = xor i8 %29, %30
  %32 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 200), align 8
  %33 = xor i8 %31, %32
  %34 = zext i8 %33 to i32
  %35 = xor i32 %11, %34
  %36 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 16), align 16
  %37 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 83), align 1
  %38 = xor i8 %37, %36
  %39 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 150), align 2
  %40 = xor i8 %38, %39
  %41 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 212), align 4
  %42 = xor i8 %40, %41
  %43 = zext i8 %42 to i32
  %44 = xor i32 %16, %43
  %45 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 32), align 32
  %46 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 96), align 32
  %47 = xor i8 %46, %45
  %48 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 160), align 32
  %49 = xor i8 %47, %48
  %50 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 236), align 4
  %51 = xor i8 %49, %50
  %52 = zext i8 %51 to i32
  %53 = xor i32 %21, %52
  %54 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 48), align 16
  %55 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 112), align 16
  %56 = xor i8 %55, %54
  %57 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 187), align 1
  %58 = xor i8 %56, %57
  %59 = load volatile i8, ptr getelementptr inbounds ([256 x i8], ptr @aes_inv_sbox, i32 0, i32 247), align 1
  %60 = xor i8 %58, %59
  %61 = zext i8 %60 to i32
  %62 = xor i32 %26, %61
  %63 = add nuw nsw i32 %8, 4
  br label %64

64:                                               ; preds = %299, %3
  %65 = phi i32 [ %35, %3 ], [ %326, %299 ]
  %66 = phi i32 [ %44, %3 ], [ %377, %299 ]
  %67 = phi i32 [ %53, %3 ], [ %428, %299 ]
  %68 = phi i32 [ %62, %3 ], [ %479, %299 ]
  %69 = phi ptr [ %5, %3 ], [ %481, %299 ]
  %70 = phi i32 [ 0, %3 ], [ %480, %299 ]
  %71 = and i32 %65, 255
  %72 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %71
  %73 = load volatile i8, ptr %72, align 1
  %74 = zext i8 %73 to i32
  %75 = lshr i32 %68, 8
  %76 = and i32 %75, 255
  %77 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %76
  %78 = load volatile i8, ptr %77, align 1
  %79 = zext i8 %78 to i32
  %80 = shl nuw nsw i32 %79, 8
  %81 = or i32 %80, %74
  %82 = lshr i32 %67, 16
  %83 = and i32 %82, 255
  %84 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %83
  %85 = load volatile i8, ptr %84, align 1
  %86 = zext i8 %85 to i32
  %87 = shl nuw nsw i32 %86, 16
  %88 = or i32 %81, %87
  %89 = lshr i32 %66, 24
  %90 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %89
  %91 = load volatile i8, ptr %90, align 1
  %92 = zext i8 %91 to i32
  %93 = shl nuw i32 %92, 24
  %94 = or i32 %88, %93
  %95 = shl i32 %94, 2
  %96 = and i32 %95, -50529028
  %97 = lshr i32 %94, 7
  %98 = and i32 %97, 16843009
  %99 = mul nuw nsw i32 %98, 54
  %100 = xor i32 %99, %96
  %101 = lshr i32 %94, 6
  %102 = and i32 %101, 16843009
  %103 = mul nuw nsw i32 %102, 27
  %104 = xor i32 %100, %103
  %105 = xor i32 %104, %94
  %106 = tail call i32 @llvm.fshl.i32(i32 %104, i32 %104, i32 16) #2
  %107 = xor i32 %105, %106
  %108 = shl i32 %107, 1
  %109 = and i32 %108, -16843010
  %110 = lshr i32 %107, 7
  %111 = and i32 %110, 16843009
  %112 = mul nuw nsw i32 %111, 27
  %113 = tail call i32 @llvm.fshl.i32(i32 %107, i32 %107, i32 16) #2
  %114 = xor i32 %109, %113
  %115 = xor i32 %114, %112
  %116 = xor i32 %115, %107
  %117 = tail call i32 @llvm.fshl.i32(i32 %116, i32 %116, i32 24) #2
  %118 = load i32, ptr %69, align 4
  %119 = xor i32 %115, %118
  %120 = xor i32 %119, %117
  %121 = and i32 %66, 255
  %122 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %121
  %123 = load volatile i8, ptr %122, align 1
  %124 = zext i8 %123 to i32
  %125 = lshr i32 %65, 8
  %126 = and i32 %125, 255
  %127 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %126
  %128 = load volatile i8, ptr %127, align 1
  %129 = zext i8 %128 to i32
  %130 = shl nuw nsw i32 %129, 8
  %131 = or i32 %130, %124
  %132 = lshr i32 %68, 16
  %133 = and i32 %132, 255
  %134 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %133
  %135 = load volatile i8, ptr %134, align 1
  %136 = zext i8 %135 to i32
  %137 = shl nuw nsw i32 %136, 16
  %138 = or i32 %131, %137
  %139 = lshr i32 %67, 24
  %140 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %139
  %141 = load volatile i8, ptr %140, align 1
  %142 = zext i8 %141 to i32
  %143 = shl nuw i32 %142, 24
  %144 = or i32 %138, %143
  %145 = shl i32 %144, 2
  %146 = and i32 %145, -50529028
  %147 = lshr i32 %144, 7
  %148 = and i32 %147, 16843009
  %149 = mul nuw nsw i32 %148, 54
  %150 = xor i32 %149, %146
  %151 = lshr i32 %144, 6
  %152 = and i32 %151, 16843009
  %153 = mul nuw nsw i32 %152, 27
  %154 = xor i32 %150, %153
  %155 = xor i32 %154, %144
  %156 = tail call i32 @llvm.fshl.i32(i32 %154, i32 %154, i32 16) #2
  %157 = xor i32 %155, %156
  %158 = shl i32 %157, 1
  %159 = and i32 %158, -16843010
  %160 = lshr i32 %157, 7
  %161 = and i32 %160, 16843009
  %162 = mul nuw nsw i32 %161, 27
  %163 = tail call i32 @llvm.fshl.i32(i32 %157, i32 %157, i32 16) #2
  %164 = xor i32 %159, %163
  %165 = xor i32 %164, %162
  %166 = xor i32 %165, %157
  %167 = tail call i32 @llvm.fshl.i32(i32 %166, i32 %166, i32 24) #2
  %168 = getelementptr i32, ptr %69, i32 1
  %169 = load i32, ptr %168, align 4
  %170 = xor i32 %165, %169
  %171 = xor i32 %170, %167
  %172 = and i32 %67, 255
  %173 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %172
  %174 = load volatile i8, ptr %173, align 1
  %175 = zext i8 %174 to i32
  %176 = lshr i32 %66, 8
  %177 = and i32 %176, 255
  %178 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %177
  %179 = load volatile i8, ptr %178, align 1
  %180 = zext i8 %179 to i32
  %181 = shl nuw nsw i32 %180, 8
  %182 = or i32 %181, %175
  %183 = lshr i32 %65, 16
  %184 = and i32 %183, 255
  %185 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %184
  %186 = load volatile i8, ptr %185, align 1
  %187 = zext i8 %186 to i32
  %188 = shl nuw nsw i32 %187, 16
  %189 = or i32 %182, %188
  %190 = lshr i32 %68, 24
  %191 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %190
  %192 = load volatile i8, ptr %191, align 1
  %193 = zext i8 %192 to i32
  %194 = shl nuw i32 %193, 24
  %195 = or i32 %189, %194
  %196 = shl i32 %195, 2
  %197 = and i32 %196, -50529028
  %198 = lshr i32 %195, 7
  %199 = and i32 %198, 16843009
  %200 = mul nuw nsw i32 %199, 54
  %201 = xor i32 %200, %197
  %202 = lshr i32 %195, 6
  %203 = and i32 %202, 16843009
  %204 = mul nuw nsw i32 %203, 27
  %205 = xor i32 %201, %204
  %206 = xor i32 %205, %195
  %207 = tail call i32 @llvm.fshl.i32(i32 %205, i32 %205, i32 16) #2
  %208 = xor i32 %206, %207
  %209 = shl i32 %208, 1
  %210 = and i32 %209, -16843010
  %211 = lshr i32 %208, 7
  %212 = and i32 %211, 16843009
  %213 = mul nuw nsw i32 %212, 27
  %214 = tail call i32 @llvm.fshl.i32(i32 %208, i32 %208, i32 16) #2
  %215 = xor i32 %210, %214
  %216 = xor i32 %215, %213
  %217 = xor i32 %216, %208
  %218 = tail call i32 @llvm.fshl.i32(i32 %217, i32 %217, i32 24) #2
  %219 = getelementptr i32, ptr %69, i32 2
  %220 = load i32, ptr %219, align 4
  %221 = xor i32 %216, %220
  %222 = xor i32 %221, %218
  %223 = and i32 %68, 255
  %224 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %223
  %225 = load volatile i8, ptr %224, align 1
  %226 = zext i8 %225 to i32
  %227 = lshr i32 %67, 8
  %228 = and i32 %227, 255
  %229 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %228
  %230 = load volatile i8, ptr %229, align 1
  %231 = zext i8 %230 to i32
  %232 = shl nuw nsw i32 %231, 8
  %233 = or i32 %232, %226
  %234 = lshr i32 %66, 16
  %235 = and i32 %234, 255
  %236 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %235
  %237 = load volatile i8, ptr %236, align 1
  %238 = zext i8 %237 to i32
  %239 = shl nuw nsw i32 %238, 16
  %240 = or i32 %233, %239
  %241 = lshr i32 %65, 24
  %242 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %241
  %243 = load volatile i8, ptr %242, align 1
  %244 = zext i8 %243 to i32
  %245 = shl nuw i32 %244, 24
  %246 = or i32 %240, %245
  %247 = shl i32 %246, 2
  %248 = and i32 %247, -50529028
  %249 = lshr i32 %246, 7
  %250 = and i32 %249, 16843009
  %251 = mul nuw nsw i32 %250, 54
  %252 = xor i32 %251, %248
  %253 = lshr i32 %246, 6
  %254 = and i32 %253, 16843009
  %255 = mul nuw nsw i32 %254, 27
  %256 = xor i32 %252, %255
  %257 = xor i32 %256, %246
  %258 = tail call i32 @llvm.fshl.i32(i32 %256, i32 %256, i32 16) #2
  %259 = xor i32 %257, %258
  %260 = shl i32 %259, 1
  %261 = and i32 %260, -16843010
  %262 = lshr i32 %259, 7
  %263 = and i32 %262, 16843009
  %264 = mul nuw nsw i32 %263, 27
  %265 = tail call i32 @llvm.fshl.i32(i32 %259, i32 %259, i32 16) #2
  %266 = xor i32 %261, %265
  %267 = xor i32 %266, %264
  %268 = xor i32 %267, %259
  %269 = tail call i32 @llvm.fshl.i32(i32 %268, i32 %268, i32 24) #2
  %270 = getelementptr i32, ptr %69, i32 3
  %271 = load i32, ptr %270, align 4
  %272 = xor i32 %267, %271
  %273 = xor i32 %272, %269
  %274 = icmp eq i32 %70, %63
  %275 = and i32 %120, 255
  %276 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %275
  %277 = load volatile i8, ptr %276, align 1
  %278 = zext i8 %277 to i32
  %279 = lshr i32 %273, 8
  %280 = and i32 %279, 255
  %281 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %280
  %282 = load volatile i8, ptr %281, align 1
  %283 = zext i8 %282 to i32
  %284 = shl nuw nsw i32 %283, 8
  %285 = or i32 %284, %278
  %286 = lshr i32 %222, 16
  %287 = and i32 %286, 255
  %288 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %287
  %289 = load volatile i8, ptr %288, align 1
  %290 = zext i8 %289 to i32
  %291 = shl nuw nsw i32 %290, 16
  %292 = or i32 %285, %291
  %293 = lshr i32 %171, 24
  %294 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %293
  %295 = load volatile i8, ptr %294, align 1
  %296 = zext i8 %295 to i32
  %297 = shl nuw i32 %296, 24
  %298 = or i32 %292, %297
  br i1 %274, label %482, label %299

299:                                              ; preds = %64
  %300 = shl i32 %298, 2
  %301 = and i32 %300, -50529028
  %302 = lshr i32 %298, 7
  %303 = and i32 %302, 16843009
  %304 = mul nuw nsw i32 %303, 54
  %305 = xor i32 %304, %301
  %306 = lshr i32 %298, 6
  %307 = and i32 %306, 16843009
  %308 = mul nuw nsw i32 %307, 27
  %309 = xor i32 %305, %308
  %310 = xor i32 %309, %298
  %311 = tail call i32 @llvm.fshl.i32(i32 %309, i32 %309, i32 16) #2
  %312 = xor i32 %310, %311
  %313 = shl i32 %312, 1
  %314 = and i32 %313, -16843010
  %315 = lshr i32 %312, 7
  %316 = and i32 %315, 16843009
  %317 = mul nuw nsw i32 %316, 27
  %318 = tail call i32 @llvm.fshl.i32(i32 %312, i32 %312, i32 16) #2
  %319 = xor i32 %314, %318
  %320 = xor i32 %319, %317
  %321 = xor i32 %320, %312
  %322 = tail call i32 @llvm.fshl.i32(i32 %321, i32 %321, i32 24) #2
  %323 = getelementptr i32, ptr %69, i32 4
  %324 = load i32, ptr %323, align 4
  %325 = xor i32 %320, %324
  %326 = xor i32 %325, %322
  %327 = and i32 %171, 255
  %328 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %327
  %329 = load volatile i8, ptr %328, align 1
  %330 = zext i8 %329 to i32
  %331 = lshr i32 %120, 8
  %332 = and i32 %331, 255
  %333 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %332
  %334 = load volatile i8, ptr %333, align 1
  %335 = zext i8 %334 to i32
  %336 = shl nuw nsw i32 %335, 8
  %337 = or i32 %336, %330
  %338 = lshr i32 %273, 16
  %339 = and i32 %338, 255
  %340 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %339
  %341 = load volatile i8, ptr %340, align 1
  %342 = zext i8 %341 to i32
  %343 = shl nuw nsw i32 %342, 16
  %344 = or i32 %337, %343
  %345 = lshr i32 %222, 24
  %346 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %345
  %347 = load volatile i8, ptr %346, align 1
  %348 = zext i8 %347 to i32
  %349 = shl nuw i32 %348, 24
  %350 = or i32 %344, %349
  %351 = shl i32 %350, 2
  %352 = and i32 %351, -50529028
  %353 = lshr i32 %350, 7
  %354 = and i32 %353, 16843009
  %355 = mul nuw nsw i32 %354, 54
  %356 = xor i32 %355, %352
  %357 = lshr i32 %350, 6
  %358 = and i32 %357, 16843009
  %359 = mul nuw nsw i32 %358, 27
  %360 = xor i32 %356, %359
  %361 = xor i32 %360, %350
  %362 = tail call i32 @llvm.fshl.i32(i32 %360, i32 %360, i32 16) #2
  %363 = xor i32 %361, %362
  %364 = shl i32 %363, 1
  %365 = and i32 %364, -16843010
  %366 = lshr i32 %363, 7
  %367 = and i32 %366, 16843009
  %368 = mul nuw nsw i32 %367, 27
  %369 = tail call i32 @llvm.fshl.i32(i32 %363, i32 %363, i32 16) #2
  %370 = xor i32 %365, %369
  %371 = xor i32 %370, %368
  %372 = xor i32 %371, %363
  %373 = tail call i32 @llvm.fshl.i32(i32 %372, i32 %372, i32 24) #2
  %374 = getelementptr i32, ptr %69, i32 5
  %375 = load i32, ptr %374, align 4
  %376 = xor i32 %371, %375
  %377 = xor i32 %376, %373
  %378 = and i32 %222, 255
  %379 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %378
  %380 = load volatile i8, ptr %379, align 1
  %381 = zext i8 %380 to i32
  %382 = lshr i32 %171, 8
  %383 = and i32 %382, 255
  %384 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %383
  %385 = load volatile i8, ptr %384, align 1
  %386 = zext i8 %385 to i32
  %387 = shl nuw nsw i32 %386, 8
  %388 = or i32 %387, %381
  %389 = lshr i32 %120, 16
  %390 = and i32 %389, 255
  %391 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %390
  %392 = load volatile i8, ptr %391, align 1
  %393 = zext i8 %392 to i32
  %394 = shl nuw nsw i32 %393, 16
  %395 = or i32 %388, %394
  %396 = lshr i32 %273, 24
  %397 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %396
  %398 = load volatile i8, ptr %397, align 1
  %399 = zext i8 %398 to i32
  %400 = shl nuw i32 %399, 24
  %401 = or i32 %395, %400
  %402 = shl i32 %401, 2
  %403 = and i32 %402, -50529028
  %404 = lshr i32 %401, 7
  %405 = and i32 %404, 16843009
  %406 = mul nuw nsw i32 %405, 54
  %407 = xor i32 %406, %403
  %408 = lshr i32 %401, 6
  %409 = and i32 %408, 16843009
  %410 = mul nuw nsw i32 %409, 27
  %411 = xor i32 %407, %410
  %412 = xor i32 %411, %401
  %413 = tail call i32 @llvm.fshl.i32(i32 %411, i32 %411, i32 16) #2
  %414 = xor i32 %412, %413
  %415 = shl i32 %414, 1
  %416 = and i32 %415, -16843010
  %417 = lshr i32 %414, 7
  %418 = and i32 %417, 16843009
  %419 = mul nuw nsw i32 %418, 27
  %420 = tail call i32 @llvm.fshl.i32(i32 %414, i32 %414, i32 16) #2
  %421 = xor i32 %416, %420
  %422 = xor i32 %421, %419
  %423 = xor i32 %422, %414
  %424 = tail call i32 @llvm.fshl.i32(i32 %423, i32 %423, i32 24) #2
  %425 = getelementptr i32, ptr %69, i32 6
  %426 = load i32, ptr %425, align 4
  %427 = xor i32 %422, %426
  %428 = xor i32 %427, %424
  %429 = and i32 %273, 255
  %430 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %429
  %431 = load volatile i8, ptr %430, align 1
  %432 = zext i8 %431 to i32
  %433 = lshr i32 %222, 8
  %434 = and i32 %433, 255
  %435 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %434
  %436 = load volatile i8, ptr %435, align 1
  %437 = zext i8 %436 to i32
  %438 = shl nuw nsw i32 %437, 8
  %439 = or i32 %438, %432
  %440 = lshr i32 %171, 16
  %441 = and i32 %440, 255
  %442 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %441
  %443 = load volatile i8, ptr %442, align 1
  %444 = zext i8 %443 to i32
  %445 = shl nuw nsw i32 %444, 16
  %446 = or i32 %439, %445
  %447 = lshr i32 %120, 24
  %448 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %447
  %449 = load volatile i8, ptr %448, align 1
  %450 = zext i8 %449 to i32
  %451 = shl nuw i32 %450, 24
  %452 = or i32 %446, %451
  %453 = shl i32 %452, 2
  %454 = and i32 %453, -50529028
  %455 = lshr i32 %452, 7
  %456 = and i32 %455, 16843009
  %457 = mul nuw nsw i32 %456, 54
  %458 = xor i32 %457, %454
  %459 = lshr i32 %452, 6
  %460 = and i32 %459, 16843009
  %461 = mul nuw nsw i32 %460, 27
  %462 = xor i32 %458, %461
  %463 = xor i32 %462, %452
  %464 = tail call i32 @llvm.fshl.i32(i32 %462, i32 %462, i32 16) #2
  %465 = xor i32 %463, %464
  %466 = shl i32 %465, 1
  %467 = and i32 %466, -16843010
  %468 = lshr i32 %465, 7
  %469 = and i32 %468, 16843009
  %470 = mul nuw nsw i32 %469, 27
  %471 = tail call i32 @llvm.fshl.i32(i32 %465, i32 %465, i32 16) #2
  %472 = xor i32 %467, %471
  %473 = xor i32 %472, %470
  %474 = xor i32 %473, %465
  %475 = tail call i32 @llvm.fshl.i32(i32 %474, i32 %474, i32 24) #2
  %476 = getelementptr i32, ptr %69, i32 7
  %477 = load i32, ptr %476, align 4
  %478 = xor i32 %473, %477
  %479 = xor i32 %478, %475
  %480 = add i32 %70, 2
  %481 = getelementptr i32, ptr %69, i32 8
  br label %64

482:                                              ; preds = %64
  %483 = getelementptr i32, ptr %69, i32 4
  %484 = load i32, ptr %483, align 4
  %485 = xor i32 %298, %484
  store i32 %485, ptr %1, align 1
  %486 = and i32 %171, 255
  %487 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %486
  %488 = load volatile i8, ptr %487, align 1
  %489 = zext i8 %488 to i32
  %490 = lshr i32 %120, 8
  %491 = and i32 %490, 255
  %492 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %491
  %493 = load volatile i8, ptr %492, align 1
  %494 = zext i8 %493 to i32
  %495 = shl nuw nsw i32 %494, 8
  %496 = or i32 %495, %489
  %497 = lshr i32 %273, 16
  %498 = and i32 %497, 255
  %499 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %498
  %500 = load volatile i8, ptr %499, align 1
  %501 = zext i8 %500 to i32
  %502 = shl nuw nsw i32 %501, 16
  %503 = or i32 %496, %502
  %504 = lshr i32 %222, 24
  %505 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %504
  %506 = load volatile i8, ptr %505, align 1
  %507 = zext i8 %506 to i32
  %508 = shl nuw i32 %507, 24
  %509 = or i32 %503, %508
  %510 = getelementptr i32, ptr %69, i32 5
  %511 = load i32, ptr %510, align 4
  %512 = xor i32 %509, %511
  %513 = getelementptr i8, ptr %1, i32 4
  store i32 %512, ptr %513, align 1
  %514 = and i32 %222, 255
  %515 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %514
  %516 = load volatile i8, ptr %515, align 1
  %517 = zext i8 %516 to i32
  %518 = lshr i32 %171, 8
  %519 = and i32 %518, 255
  %520 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %519
  %521 = load volatile i8, ptr %520, align 1
  %522 = zext i8 %521 to i32
  %523 = shl nuw nsw i32 %522, 8
  %524 = or i32 %523, %517
  %525 = lshr i32 %120, 16
  %526 = and i32 %525, 255
  %527 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %526
  %528 = load volatile i8, ptr %527, align 1
  %529 = zext i8 %528 to i32
  %530 = shl nuw nsw i32 %529, 16
  %531 = or i32 %524, %530
  %532 = lshr i32 %273, 24
  %533 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %532
  %534 = load volatile i8, ptr %533, align 1
  %535 = zext i8 %534 to i32
  %536 = shl nuw i32 %535, 24
  %537 = or i32 %531, %536
  %538 = getelementptr i32, ptr %69, i32 6
  %539 = load i32, ptr %538, align 4
  %540 = xor i32 %537, %539
  %541 = getelementptr i8, ptr %1, i32 8
  store i32 %540, ptr %541, align 1
  %542 = and i32 %273, 255
  %543 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %542
  %544 = load volatile i8, ptr %543, align 1
  %545 = zext i8 %544 to i32
  %546 = lshr i32 %222, 8
  %547 = and i32 %546, 255
  %548 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %547
  %549 = load volatile i8, ptr %548, align 1
  %550 = zext i8 %549 to i32
  %551 = shl nuw nsw i32 %550, 8
  %552 = or i32 %551, %545
  %553 = lshr i32 %171, 16
  %554 = and i32 %553, 255
  %555 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %554
  %556 = load volatile i8, ptr %555, align 1
  %557 = zext i8 %556 to i32
  %558 = shl nuw nsw i32 %557, 16
  %559 = or i32 %552, %558
  %560 = lshr i32 %120, 24
  %561 = getelementptr [256 x i8], ptr @aes_inv_sbox, i32 0, i32 %560
  %562 = load volatile i8, ptr %561, align 1
  %563 = zext i8 %562 to i32
  %564 = shl nuw i32 %563, 24
  %565 = or i32 %559, %564
  %566 = getelementptr i32, ptr %69, i32 7
  %567 = load i32, ptr %566, align 4
  %568 = xor i32 %565, %567
  %569 = getelementptr i8, ptr %1, i32 12
  store i32 %568, ptr %569, align 1
  ret void
}

; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
declare i32 @llvm.fshl.i32(i32, i32, i32) #1

attributes #0 = { nofree nounwind null_pointer_is_valid sspstrong uwtable(sync) "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="generic" "target-features"="+armv7-a,+dsp,+read-tp-hard,+soft-float,-aes,-bf16,-d32,-dotprod,-fp-armv8,-fp-armv8d16,-fp-armv8d16sp,-fp-armv8sp,-fp16,-fp16fml,-fp64,-fpregs,-fullfp16,-mve,-mve.fp,-neon,-sha2,-thumb-mode,-vfp2,-vfp2sp,-vfp3,-vfp3d16,-vfp3d16sp,-vfp3sp,-vfp4,-vfp4d16,-vfp4d16sp,-vfp4sp" "use-soft-float"="true" "warn-stack-size"="1024" }
attributes #1 = { nocallback nofree nosync nounwind readnone speculatable willreturn }
attributes #2 = { nounwind }

!llvm.module.flags = !{!0, !1, !2, !3, !4, !5, !6}
!llvm.ident = !{!7}

!0 = !{i32 1, !"wchar_size", i32 2}
!1 = !{i32 1, !"min_enum_size", i32 4}
!2 = !{i32 8, !"branch-target-enforcement", i32 0}
!3 = !{i32 8, !"sign-return-address", i32 0}
!4 = !{i32 8, !"sign-return-address-all", i32 0}
!5 = !{i32 8, !"sign-return-address-with-bkey", i32 0}
!6 = !{i32 7, !"uwtable", i32 1}
!7 = !{!"clang version 15.0.0 (/llk/llvm-project-main/clang 126a1f78513fb688819156df98cf7ea83b5e8c18)"}
